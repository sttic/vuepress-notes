# Introduction to Linear Algebra

Vectors are n-tuples of numbers.

$$ \vec{x} =
\begin{bmatrix}
x_1\\
x_2\\
\vdots\\
x_n
\end{bmatrix}
\quad x_i \in\mathbb{R} $$

Different notations exist, such as $\vec{x} = (x_1, x_2, \ldots, x_n)$

## Operations With Vectors

### Equality

::: center
$\vec{x} = \begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix} \quad x_i \in \mathbb{R}$ and $\vec{y} = \begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix} \quad y_i \in \mathbb{R}$
:::

are equal if each corresponding value is the same.

$$ x_1 = y_1, x_1 = y_1, \ldots, x_n = y_n $$

### Addition and Subtraction

Combining vectors (recall "tip to tail").

$$
\begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix} \pm
\begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix} =
\begin{bmatrix}x_1 \pm y_1\\x_2 \pm y_2\\\vdots\\x_n \pm y_n\end{bmatrix}
$$

### Scalar Multiplication

Scaling done on the vector which only changes magnitude.
A negative refers to 'flipping' or reversing its direction.

$$ c\vec{x} = \begin{bmatrix}cx_1\\cx_2\\\vdots\\cx_n\end{bmatrix} \quad c\in\mathbb{R} $$

### Dot Product

Directional multiplication.

$$
\begin{bmatrix}x_1\\x_2\\\vdots\\x_n\end{bmatrix} \cdot
\begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix} =
x_1y_1 + x_2y_2 + \ldots + x_ny_n
$$

See [Better Explained](https://betterexplained.com/articles/vector-calculus-understanding-the-dot-product/)

### Magnitude (Length)

Essentially an n-dimensional pythagorean theorem to find the length of a vector.

$$ \|\vec{x}\| = \sqrt{x_{\tiny 1}^2 + x_{\tiny 2}^2 + \ldots + x_{\tiny n}^2} $$

## Orthogonality (Perpendicular)

$$ \vec{x}\bot\vec{y} \Leftrightarrow \vec{x}\cdot\vec{y} = 0 $$

## Angles

$$ cos(\theta) = \frac{\vec{x}\cdot\vec{y}}{\|\vec{x}\|\|\vec{y}\|} $$

For the acute angle, find $\theta$ where $0\le\theta\le\pi$.

Two vectors are parallel if $\theta = 0$ or $\theta = \pi$.

## Projections

$$ proj_{\vec{u}}\vec{v} = \frac{\vec{u}\cdot\vec{v}}{\|\vec{u}\|^2}\vec{u} $$
